{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Barcelona cultural agenda\n",
    "\n",
    "\n",
    "***\n",
    "<br/>\n",
    "\n",
    "This code scrapes Barcelona's agenda website (https://guia.barcelona.cat/ca) and gets the main information of each event for a specified set of time.\n",
    "\n",
    "calling the main function:\n",
    "\n",
    ">scrape_agenda(agenda,ndays,nr,threading = True):\n",
    "\n",
    ">agenda: _dataframe_ with the columns as the agenda.csv file explained below.\n",
    ">\n",
    ">ndays: _integer_ starting from current date, number of days you want to retrieve the agenda for.\n",
    ">\n",
    ">nr: _integer_ number of events per day\n",
    ">\n",
    ">threading: True concurrency enabled. False otherwise.\n",
    "\n",
    ">returns: _dataframe_  with the detailed information of each event for the ndays after current date with the same    format as agenda   \n",
    "\n",
    "The returned data frame is stored as \"agenda.csv\" file with the following attributes:\n",
    " \n",
    " - Event_Name: Title of the event\n",
    " - Starting_from: Starting date or Permanent event\n",
    " - Ending: Ending date or \"-\" if Permanent event or day event\n",
    " - Location: Venue\n",
    " - Address: Address of the venue\n",
    " - Description: Short description of the event\n",
    " - Link: Link to the specific event page from which you can get more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import time\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# Initialization #############################################\n",
    "\n",
    "#Guia Barcelona Website ordered by popularity\n",
    "website = \"https://guia.barcelona.cat/ca/llistat?pg=search&cerca=*:*&tr=619&af=code_prop&c=00619*&dt=2018-11-02,2018-11-02&nr=10&sort=popularity,desc\"\n",
    "\n",
    "#Number of selected days \n",
    "ndays = 7\n",
    "\n",
    "#Number of events per day [10,15,20,25...] only multiple of 5 allowed\n",
    "nr = 15\n",
    "\n",
    "#Defining agenda format\n",
    "agenda = pd.DataFrame(columns = [\"Event_Name\",\"Starting_from\",\"Ending\",\"Location\",\"Address\",\"Description\",\"Link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The url from which the code starts is a random selected day agenda whose results are ordered by popularity. This url is then customized for the selected set of days and the number of events per day.\n",
    "\n",
    "By default, number of days for which we extract data is 7 and number of events per day is 15 but this can be customized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# Functions #############################################\n",
    "\n",
    "\n",
    "def get_period(ndays = 7):\n",
    "    \"\"\"\n",
    "    ndays is an integer meaning the number of days you want to get the agenda for.\n",
    "    returns a list of ndays dates as string with format 'YYYY-MM-DD'.\n",
    "    \"\"\"\n",
    "    period = []\n",
    "    for i in range(ndays):\n",
    "        period.append((dt.date.today() + dt.timedelta(days =i)).isoformat())\n",
    "    return period\n",
    "\n",
    "\n",
    "#---------------------------------------------#\n",
    "\n",
    "def geturl_day (day,nr = 15):\n",
    "    \"\"\"\n",
    "    day is a string with format 'DD-MM-YYYY'.\n",
    "    nr is an integer defining how many evens we want per day.\n",
    "    returns the url of barcelona agenda webpage for the especific day as string.    \n",
    "    \"\"\"\n",
    "    webparams = website.split(\"&\")\n",
    "    webparams[5] = \"dt=\" + day + \",\" + day\n",
    "    webparams[6] = \"nr=\" + str(nr)\n",
    "    return \"&\".join(webparams)\n",
    "\n",
    "\n",
    "#---------------------------------------------#\n",
    "\n",
    "# Get period urls\n",
    "def get_period_urls (period,nr):\n",
    "    \"\"\" \n",
    "    period is a list of strings with format 'DD-MM-YYYY'.\n",
    "    returns a list of urls for each day/date within the period. \n",
    "    \"\"\"\n",
    "    period_urls = []\n",
    "    for day in period:\n",
    "        period_urls.append(geturl_day(day,nr))\n",
    "    \n",
    "    return period_urls\n",
    "        \n",
    "#---------------------------------------------#\n",
    "    \n",
    "def get_parsed_html(url):    \n",
    "    \"\"\" returns the parsed html url \"\"\"\n",
    "    r = requests.get(url)\n",
    "    return BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "#---------------------------------------------#\n",
    "\n",
    "def find_dates (any_str):\n",
    "    \"\"\"\n",
    "    any_str is a string \n",
    "    returns a list of strings with format DD/MM/YYY\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\d{2}/\\d{2}/\\d{4}',any_str)\n",
    "\n",
    "\n",
    "#---------------------------------------------#\n",
    "\n",
    "\n",
    "def get_description_link (item):\n",
    "    \"\"\"\n",
    "    item is a b4s object containing all the information of one event\n",
    "    returns the link to the webpage with futher information and a short description of the event\n",
    "    \"\"\"\n",
    "    desc_url = \"https://guia.barcelona.cat/ca/\"+item.h3.a['href']\n",
    "    soup_desc = get_parsed_html(desc_url)\n",
    "    item_desc = soup_desc.select(\".entradeta\")\n",
    "    return desc_url , item_desc[0].get_text()\n",
    "\n",
    "\n",
    "#---------------------------------------------#\n",
    "\n",
    "def get_event (item):\n",
    "    \"\"\"\n",
    "    item is a b4s object containing all the information of one event\n",
    "    returns a dictionary with event information:  Event_Name, Location, Starting_from date, Ending date, \n",
    "                                                  Address, Description ,Link\n",
    "    \"\"\"\n",
    "    event = {}\n",
    "    event[\"Event_Name\"] =  item.h3.a.get_text()\n",
    "    \n",
    "    # The html code for event details is coded as a html description list.\n",
    "    # Depending on the list term, the discription is retrieved from next sibling.\n",
    "    for sibling in item.dl.children:\n",
    "        if sibling.get_text() == \"On:\":\n",
    "            event[\"Location\"] = sibling.next_sibling.get_text()\n",
    "        elif sibling.get_text() == \"Quan:\":\n",
    "            dates = find_dates(sibling.next_sibling.get_text())\n",
    "            if len(dates) == 1:\n",
    "                event[\"Starting_from\"] = dates[0]\n",
    "                event[\"Ending\"] = \"-\"\n",
    "            elif len(dates) == 2:\n",
    "                event[\"Starting_from\"] = dates[0]\n",
    "                event[\"Ending\"] = dates[1]\n",
    "            else:\n",
    "                event[\"Starting_from\"] = sibling.next_sibling.get_text()\n",
    "                event[\"Ending\"] = \"-\"\n",
    "        elif sibling.get_text() == \"Adre√ßa:\":\n",
    "            event[\"Address\"] = sibling.next_sibling.get_text()\n",
    "        else:\n",
    "            pass\n",
    "        # The link and Description of the event are retrieved from a subpage \n",
    "        event[\"Link\"],event[\"Description\"] = get_description_link(item)\n",
    "    \n",
    "    return event\n",
    "\n",
    "\n",
    "#---------------------------------------------#\n",
    "\n",
    "\n",
    "def event_in_agenda (agenda,event):\n",
    "    \"\"\"\n",
    "    agenda is a data frame with columns Event_Name, Location, Starting_from date, Ending date, \n",
    "                                                  Address, Description ,Link\n",
    "    event is a dictionary with event information: Event_Name, Location, Starting_from date, Ending date, \n",
    "                                                  Address, Description ,Link\n",
    "    returns true if the event already in the agenda.\n",
    "    \"\"\"\n",
    "    if event[\"Event_Name\"] in agenda.Event_Name.values:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#---------------------------------------------#\n",
    "    \n",
    "def add_day_agenda(agenda,url):\n",
    "    \"\"\"\n",
    "    agenda is a data frame with columns Event_Name, Location, Starting_from date, Ending date, Address and description\n",
    "    url from which we extract agenda of a specific day\n",
    "    returns the agenda with the new events included if the event is not in the df yet. Returns the agenda as is otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = get_parsed_html(url)\n",
    "    items = soup.select(\".dades\")\n",
    "    \n",
    "    # Implementing concurrent threads for getting event information.\n",
    "    # get_event function takes some information from da agenda url and goes to the detailed url for each event\n",
    "    # to retrieve further event data.\n",
    "    # The html requests for each event have a lot of waiting time involved, that's the reason \n",
    "    # concurrent thread was implemented.\n",
    "    \n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        evs = list(pool.map(get_event, items))\n",
    "    \n",
    "    # Adds the event to the agenda df if event is not already there.    \n",
    "    for ev in evs:     \n",
    "        if not event_in_agenda(agenda,ev) :\n",
    "            agenda = agenda.append(other = ev,ignore_index = True)\n",
    "    return agenda\n",
    "\n",
    "def add_day_agenda_wo_scraping(agenda,url):\n",
    "    \"\"\"\n",
    "    same function as the above one but without threading implemented\n",
    "    \"\"\"\n",
    "    soup = get_parsed_html(url)\n",
    "    items = soup.select(\".dades\")\n",
    "                        \n",
    "    for item in items:\n",
    "        ev = get_event(item)\n",
    "        if not event_in_agenda(agenda,ev):\n",
    "            agenda = agenda.append(other = ev,ignore_index = True)\n",
    "    return agenda\n",
    "\n",
    "def scrape_agenda(agenda,ndays,nr,threading = True):\n",
    "    \"\"\"\n",
    "    agenda dataframe \n",
    "    ndays integer number of days\n",
    "    nr integer number of events per day\n",
    "    returns an agenda dframe with the detailed information of each event for the ndays after current date.    \n",
    "    \"\"\"\n",
    "    urls = get_period_urls(get_period(ndays),nr)\n",
    "    \n",
    "    #Threading enabled\n",
    "    if threading :\n",
    "        for url in urls:\n",
    "            agenda = add_day_agenda(agenda,url)\n",
    "    \n",
    "    #Threading not enabled\n",
    "    else:\n",
    "        for url in urls:\n",
    "            agenda = add_day_agenda_wo_scraping(agenda,url)\n",
    "    return agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare algorithm performance with / with out threading, is has been implemented the option to enable/disable threading when calling the main function.\n",
    "\n",
    "Below, as an example, can be found the comparison in time between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process time:  60  seconds\n"
     ]
    }
   ],
   "source": [
    "# Scraping agenda Barcelona with threading\n",
    "start_time = time.time()\n",
    "\n",
    "agenda = scrape_agenda(agenda,ndays,nr,threading = True)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Process time: \", round(end_time-start_time),  \" seconds\")\n",
    "\n",
    "agenda.to_csv(\"./agenda.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process time:  229  seconds\n"
     ]
    }
   ],
   "source": [
    "# Scraping agenda Barcelona without threading\n",
    "agenda = pd.DataFrame(columns = [\"Event_Name\",\"Starting_from\",\"Ending\",\"Location\",\"Address\",\"Description\",\"Link\"])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "agenda = scrape_agenda(agenda,ndays,nr,threading = False)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Process time: \", round(end_time-start_time),  \" seconds\")\n",
    "\n",
    "agenda.to_csv(\"./agenda.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "362.85px",
    "left": "1548px",
    "right": "20px",
    "top": "120px",
    "width": "352px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
